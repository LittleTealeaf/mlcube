{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "[Reinforcement Learning Article](https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import Random\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Methods\n",
    "\n",
    "So I did some syntax lessons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Move:\n",
    "    def __init__(\n",
    "        self, name: str, loops: list[list[int]], two: bool = False, prime: bool = False\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.matrix: np.ndarray = np.identity(9 * 6, dtype=np.int8)\n",
    "        for loop in loops:\n",
    "            first = np.copy(self.matrix[loop[0]])\n",
    "            for i in range(len(loop) - 1):\n",
    "                self.matrix[loop[i]] = self.matrix[loop[i + 1]]\n",
    "            self.matrix[loop[-1]] = first\n",
    "        if two:\n",
    "            self.matrix = self.matrix @ self.matrix\n",
    "        if prime:\n",
    "            self.matrix = self.matrix.T\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Move: {self.name}\"\n",
    "\n",
    "\n",
    "def build_moves(letter: str, loops: list[list[int]]) -> list[Move]:\n",
    "    return [\n",
    "        Move(letter, loops),\n",
    "        Move(f\"{letter}P\", loops, prime=True),\n",
    "        Move(f\"{letter}2\", loops, two=True),\n",
    "    ]\n",
    "\n",
    "\n",
    "# Behold, python syntax\n",
    "MOVES = [\n",
    "    move\n",
    "    for moves in [\n",
    "        build_moves(\n",
    "            \"R\",\n",
    "            [\n",
    "                [20, 2, 42, 47],\n",
    "                [23, 5, 39, 50],\n",
    "                [26, 8, 36, 53],\n",
    "                [27, 29, 35, 33],\n",
    "                [28, 32, 34, 30],\n",
    "            ],\n",
    "        ),\n",
    "        build_moves(\n",
    "            \"U\",\n",
    "            [\n",
    "                [20, 11, 38, 29],\n",
    "                [19, 10, 37, 28],\n",
    "                [18, 9, 36, 27],\n",
    "                [8, 6, 0, 2],\n",
    "                [7, 3, 1, 5],\n",
    "            ],\n",
    "        ),\n",
    "        build_moves(\n",
    "            \"L\",\n",
    "            [\n",
    "                [18, 45, 44, 0],\n",
    "                [21, 48, 41, 3],\n",
    "                [24, 51, 38, 6],\n",
    "                [11, 17, 15, 9],\n",
    "                [14, 16, 12, 10],\n",
    "            ],\n",
    "        ),\n",
    "        build_moves(\n",
    "            \"D\",\n",
    "            [\n",
    "                [24, 33, 42, 15],\n",
    "                [25, 34, 43, 16],\n",
    "                [26, 35, 44, 17],\n",
    "                [45, 47, 53, 51],\n",
    "                [46, 50, 52, 48],\n",
    "            ],\n",
    "        ),\n",
    "        build_moves(\n",
    "            \"F\",\n",
    "            [\n",
    "                [6, 27, 47, 17],\n",
    "                [7, 30, 46, 14],\n",
    "                [8, 33, 45, 11],\n",
    "                [18, 20, 26, 24],\n",
    "                [19, 23, 25, 21],\n",
    "            ],\n",
    "        ),\n",
    "        build_moves(\n",
    "            \"B\",\n",
    "            [\n",
    "                [36, 38, 44, 42],\n",
    "                [37, 41, 43, 39],\n",
    "                [29, 0, 15, 53],\n",
    "                [32, 1, 12, 52],\n",
    "                [35, 2, 9, 51],\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    for move in moves\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cube Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cube():\n",
    "    state = np.zeros((9 * 6), dtype=np.int8)\n",
    "    for i in range(state.size):\n",
    "        state[i] = i / 9\n",
    "    return state\n",
    "\n",
    "\n",
    "def apply_move(state, move: Move) -> np.ndarray:\n",
    "    return state @ move.matrix\n",
    "\n",
    "\n",
    "def scramble(state: np.ndarray, count: int) -> np.ndarray:\n",
    "    random = Random()\n",
    "    return state @ reduce(\n",
    "        lambda a, b: a @ b, [random.choice(MOVES).matrix for i in range(count)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The chance that the agent will choose to explore instead of picking the best answer'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPSILON = 0.5\n",
    "\"The chance that the agent will choose to explore instead of picking the best answer\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting State to Vector\n",
    "\n",
    "In order to make an accurate network, we will need to convert the cube's state array to a longer array to make it clearer to the network what color is where\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_vector(state):\n",
    "    vector = np.zeros((9 * 6 * 6,1),dtype=np.float32)\n",
    "    for i in range(9 * 6):\n",
    "        color = state[i]\n",
    "        vector[i * 6 + color] = 1\n",
    "    return vector.T\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_network(sizes: list[int]) -> list[tuple[(tf.Variable, tf.Variable)]]:\n",
    "    sizes = sizes + [len(MOVES)]\n",
    "    values = []\n",
    "    for i in range(len(sizes)):\n",
    "        size = sizes[i]\n",
    "        prev_size = 9 * 6 * 6\n",
    "        if i > 0:\n",
    "            prev_size = sizes[i - 1]\n",
    "        weights = tf.Variable(\n",
    "            tf.random.normal([prev_size, size], stddev=0.03), name=f\"W{i+1}\"\n",
    "        )\n",
    "        constants = tf.Variable(tf.random.normal([size]), name=f\"b{i+1}\")\n",
    "        values.append((weights, constants))\n",
    "    return values\n",
    "\n",
    "\n",
    "def feed_network(state, network: list[tuple[(tf.Variable, tf.Variable)]]):\n",
    "    x = tf.cast(state, tf.float32)\n",
    "    for i in range(len(network)):\n",
    "        W, b = network[i]\n",
    "        if i > 0:\n",
    "            x = tf.nn.relu(x)\n",
    "        x = tf.add(tf.matmul(x, W), b)\n",
    "    return x\n",
    "\n",
    "\n",
    "def copy_network(network: list[tuple[(tf.Variable, tf.Variable)]]):\n",
    "    copy = []\n",
    "    for layer in network:\n",
    "        W, b = layer\n",
    "        copy.append((np.copy(W), np.copy(b)))\n",
    "    return copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state: np.ndarray):\n",
    "    value = 0\n",
    "    for i in range(9 * 6):\n",
    "        if state[i] == i // 9:\n",
    "            value = value + 1\n",
    "        else:\n",
    "            value = value - 1\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replay(\n",
    "    network: list[tuple[(tf.Variable, tf.Variable)]],\n",
    "    count: int,\n",
    "    epsilon: float = EPSILON,\n",
    "):\n",
    "    replays: list[\n",
    "        tuple[\n",
    "            (\n",
    "                np.ndarray,  # current state\n",
    "                int,  # action\n",
    "                np.ndarray,  # next state\n",
    "                tf.float32,  # Q-Value\n",
    "            )\n",
    "        ]\n",
    "    ] = []\n",
    "\n",
    "    random = Random()\n",
    "\n",
    "    cube = scramble(new_cube(), 10000)\n",
    "\n",
    "    for i in range(count):\n",
    "        choice: int = -1\n",
    "        if random.random() < epsilon:\n",
    "            choice = random.randrange(0, len(MOVES))\n",
    "        else:\n",
    "            q_vals = feed_network(state_to_vector(cube), network)\n",
    "            index_max = tf.argmax(q_vals, 1).numpy()[0]\n",
    "            choice = index_max\n",
    "        new_state = apply_move(cube, MOVES[choice])\n",
    "\n",
    "        replays.append(\n",
    "            (\n",
    "                state_to_vector(cube),\n",
    "                choice,\n",
    "                state_to_vector(new_state),\n",
    "                get_reward(new_state),\n",
    "            )\n",
    "        )\n",
    "        cube = new_state\n",
    "\n",
    "    return replays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Predictor\n",
    "\n",
    "This function tests how well the network runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(network):\n",
    "\n",
    "    count = 50\n",
    "    total_value = 0\n",
    "    for i in range(count):\n",
    "        cube = scramble(new_cube(), 100)\n",
    "        count = 0\n",
    "        while count < 100 and get_reward(cube) < 9 * 6:\n",
    "            count: int = count + 1\n",
    "            vals = feed_network(state_to_vector(cube), network)\n",
    "            apply_move(cube, MOVES[tf.argmax(vals)[0]])\n",
    "\n",
    "        total_value: int = total_value + get_reward(cube)\n",
    "    return total_value / (9 * 6 * count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Operation\n",
    "\n",
    "I think this is what it does\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(\n",
    "    network: list[tuple[(tf.Variable, tf.Variable)]],\n",
    "    target: list[tuple[(tf.Variable, tf.Variable)]],\n",
    "    replays: list[\n",
    "        tuple[\n",
    "            (\n",
    "                np.ndarray,\n",
    "                int,\n",
    "                np.ndarray,\n",
    "                tf.float32,\n",
    "            )\n",
    "        ]\n",
    "    ],\n",
    "    lr_schedule=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.9\n",
    "    ),\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # I HAVE NO IDEA\n",
    "        trainable_variables = [var for vars in network for var in vars]\n",
    "\n",
    "        for variable in trainable_variables:\n",
    "            tape.watch(variable)\n",
    "\n",
    "        action = [replay[1] for replay in replays]\n",
    "        for i in range(len(action)):\n",
    "            tmp = np.zeros((1, len(MOVES)), dtype=np.float32)\n",
    "            tmp[0][action[i]] = 1.0\n",
    "            action[i] = tmp.T\n",
    "\n",
    "        state_1 = tf.constant([replay[0] for replay in replays], dtype=tf.float32)\n",
    "        action = tf.constant(action, dtype=tf.float32)\n",
    "        state_2 = tf.constant([replay[2] for replay in replays], dtype=tf.float32)\n",
    "        reward = tf.constant([replay[3] for replay in replays], dtype=tf.float32)\n",
    "\n",
    "        # Calculates Q values of the first state\n",
    "        state_1_q = feed_network(state_1, network)\n",
    "\n",
    "        # makes a selection matrix for state_1\n",
    "        state_1_max = tf.matmul(state_1_q, action)[:, 0, 0]\n",
    "\n",
    "        # gets the Q value of the selected action\n",
    "        state_2_q = feed_network(state_2, target)\n",
    "\n",
    "        state_2_max = tf.argmax(state_2_q, axis=2)\n",
    "\n",
    "        predicted_q = state_1_max\n",
    "\n",
    "        target_q = tf.add(reward, tf.cast(state_2_max[:, 0], dtype=tf.float32))\n",
    "        loss = tf.square(target_q - predicted_q)\n",
    "\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        opt.apply_gradients(zip(gradients, trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing and Retrieving State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_network(network):\n",
    "    data = [\n",
    "        {\n",
    "            'W': W.numpy().tolist(),\n",
    "            'B': B.numpy().tolist()\n",
    "        }\n",
    "        for (W,B) in network\n",
    "    ]\n",
    "    return data\n",
    "\n",
    "def restore_network(serialized):\n",
    "    return [\n",
    "        (\n",
    "            tf.Variable(A['W']),\n",
    "            tf.Variable(A['B'])\n",
    "        )\n",
    "        for A in serialized\n",
    "    ]\n",
    "\n",
    "def load_json(name):\n",
    "    with open(name) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(name,data):\n",
    "    with open(name,'w') as f:\n",
    "        f.write(json.dumps(data,indent=2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OK, LETS SEE...\n",
    "\n",
    "# network = random_network([10, 10])\n",
    "# target = copy_network(network)\n",
    "# Attempts to restore the network from a file\n",
    "\n",
    "network = random_network([10,15])\n",
    "\n",
    "if os.path.exists('./network.json'):\n",
    "    network = restore_network(load_json('./network.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Accuracy: -0.24407407407407408, updating learning rate 0.007738601508916324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Github\\mlcube\\notebook.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m replay_sample \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(replay, batch_sample_size)\n\u001b[0;32m     <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m DQN(network, target, replay_sample,lr_schedule\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m---> <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m value \u001b[39m=\u001b[39m accuracy(network)\n\u001b[0;32m     <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Update learning rate\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m*\u001b[39m (value \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[1;32mf:\\Github\\mlcube\\notebook.ipynb Cell 27\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(network)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         count: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         vals \u001b[39m=\u001b[39m feed_network(state_to_vector(cube), network)\n\u001b[1;32m---> <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         apply_move(cube, MOVES[tf\u001b[39m.\u001b[39;49margmax(vals)[\u001b[39m0\u001b[39m]])\n\u001b[0;32m     <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     total_value: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m total_value \u001b[39m+\u001b[39m get_reward(cube)\n\u001b[0;32m     <a href='vscode-notebook-cell://tunneling%2Bteatopwindows/f%3A/Github/mlcube/notebook.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mreturn\u001b[39;00m total_value \u001b[39m/\u001b[39m (\u001b[39m9\u001b[39m \u001b[39m*\u001b[39m \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m count)\n",
      "File \u001b[1;32mc:\\Users\\Littl\\anaconda3\\envs\\mlcube\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Littl\\anaconda3\\envs\\mlcube\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Littl\\anaconda3\\envs\\mlcube\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:301\u001b[0m, in \u001b[0;36margmax_v2\u001b[1;34m(input, axis, output_type, name)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m   axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49marg_max(\u001b[39minput\u001b[39;49m, axis, name\u001b[39m=\u001b[39;49mname, output_type\u001b[39m=\u001b[39;49moutput_type)\n",
      "File \u001b[1;32mc:\\Users\\Littl\\anaconda3\\envs\\mlcube\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:823\u001b[0m, in \u001b[0;36marg_max\u001b[1;34m(input, dimension, output_type, name)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    822\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    824\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mArgMax\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, dimension, \u001b[39m\"\u001b[39;49m\u001b[39moutput_type\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_type)\n\u001b[0;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    826\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 10_000\n",
    "batch_sample_size = 1_000\n",
    "batch_count = 1_000\n",
    "target_update_interval = 5\n",
    "\n",
    "random = Random()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-2, decay_steps=batch_count, decay_rate=0.9\n",
    "    )\n",
    "\n",
    "network = random_network([10,15])\n",
    "\n",
    "if os.path.exists('./network.json'):\n",
    "    network = restore_network(load_json('./network.json'))\n",
    "\n",
    "save_json('./network.json',store_network(network))\n",
    "\n",
    "# Learning rate is a number between 0 and 1. It will be updated based on how accurate the network currently is. \n",
    "# For example, if the network accuracy is close to -1, then it will have a high learning rate in the next iteration, and vice versa\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(batch_count):\n",
    "    if i % target_update_interval == 0:\n",
    "        target = copy_network(network)\n",
    "    replay = create_replay(network, batch_size, epsilon=i / batch_count)\n",
    "    replay_sample = random.sample(replay, batch_sample_size)\n",
    "    DQN(network, target, replay_sample,lr_schedule=learning_rate)\n",
    "    value = accuracy(network)\n",
    "\n",
    "    # Update learning rate\n",
    "    learning_rate = (-1.0 * (value - 1) / 2 / 10)**2 * 2\n",
    "\n",
    "    save_json('./network.json',store_network(network))\n",
    "    print(f\"Batch {i}: Accuracy: {value}, updating learning rate {learning_rate}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mlcube')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7641580b910c9b01375a0fd9701e80f509c3f72b49a4ffa822590e7a07fce613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
